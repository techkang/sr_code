import functools
import math
import sys
from collections import OrderedDict
from collections.abc import Iterable
from itertools import repeat

import numpy as np
import torch as t
import torch.nn as nn
import torch.nn.init as init
from scipy.ndimage import gaussian_filter


def weights_init_normal(m, std=0.02):
    classname = m.__class__.__name__
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
        if classname != "MeanShift":
            print('initializing [%s] ...' % classname)
            init.normal_(m.weight.data, 0.0, std)
            if m.bias is not None:
                m.bias.data.zero_()
    elif isinstance(m, nn.Linear):
        init.normal_(m.weight.data, 0.0, std)
        if m.bias is not None:
            m.bias.data.zero_()
    elif isinstance(m, nn.BatchNorm2d):
        init.normal_(m.weight.data, 1.0, std)
        init.constant_(m.bias.data, 0.0)


def weights_init_kaiming(m, scale=1):
    classname = m.__class__.__name__
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
        if classname != "MeanShift":
            print('initializing [%s] ...' % classname)
            init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
            m.weight.data *= scale
            if m.bias is not None:
                m.bias.data.zero_()
    elif isinstance(m, nn.Linear):
        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
        m.weight.data *= scale
        if m.bias is not None:
            m.bias.data.zero_()
    elif isinstance(m, nn.BatchNorm2d):
        init.constant_(m.weight.data, 1.0)
        m.weight.data *= scale
        init.constant_(m.bias.data, 0.0)


def weights_init_orthogonal(m):
    classname = m.__class__.__name__
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
        if classname != "MeanShift":
            print('initializing [%s] ...' % classname)
            init.orthogonal_(m.weight.data, gain=1)
            if m.bias is not None:
                m.bias.data.zero_()
    elif isinstance(m, nn.Linear):
        init.orthogonal_(m.weight.data, gain=1)
        if m.bias is not None:
            m.bias.data.zero_()
    elif isinstance(m, nn.BatchNorm2d):
        init.normal_(m.weight.data, 1.0, 0.02)
        init.constant_(m.bias.data, 0.0)


def init_weights(net, init_type='kaiming', scale=1, std=0.02):
    # scale for 'kaiming', std for 'normal'.
    print('initialization method [%s]' % init_type)
    if init_type == 'normal':
        weights_init_normal_ = functools.partial(weights_init_normal, std=std)
        net.apply(weights_init_normal_)
    elif init_type == 'kaiming':
        weights_init_kaiming_ = functools.partial(weights_init_kaiming, scale=scale)
        net.apply(weights_init_kaiming_)
    elif init_type == 'orthogonal':
        net.apply(weights_init_orthogonal)
    else:
        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)


def activation(act_type='relu', inplace=False, slope=0.2, n_prelu=1):
    act_type = act_type.lower()
    if act_type == 'relu':
        layer = nn.ReLU(inplace)
    elif act_type == 'lrelu':
        layer = nn.LeakyReLU(slope, inplace)
    elif act_type == 'prelu':
        layer = nn.PReLU(num_parameters=n_prelu, init=slope)
    else:
        raise NotImplementedError('[ERROR] Activation layer [%s] is not implemented!' % act_type)
    return layer


def norm(n_feature, norm_type='bn'):
    norm_type = norm_type.lower()
    if norm_type == 'bn':
        layer = nn.BatchNorm2d(n_feature)
    else:
        raise NotImplementedError('[ERROR] Normalization layer [%s] is not implemented!' % norm_type)
    return layer


def pad(pad_type, padding):
    pad_type = pad_type.lower()
    if padding == 0:
        return None

    if pad_type == 'reflect':
        layer = nn.ReflectionPad2d(padding)
    elif pad_type == 'replicate':
        layer = nn.ReplicationPad2d(padding)
    else:
        raise NotImplementedError('[ERROR] Padding layer [%s] is not implemented!' % pad_type)
    return layer


def sequential(*args):  # TODO: remove it
    if len(args) == 1:
        if isinstance(args[0], OrderedDict):
            raise NotImplementedError('[ERROR] %s.sequential() does not support OrderedDict' % sys.modules[__name__])
        else:
            return args[0]
    modules = []
    for module in args:
        if isinstance(module, nn.Sequential):
            for submodule in module:
                modules.append(submodule)
        elif isinstance(module, nn.Module):
            modules.append(module)
    return nn.Sequential(*modules)


class BasicBlock(nn.Module):
    def __init__(self, in_channel, out_channel, kernel_size, *, conv='conv', stride=1, padding=None, act=None, bn=False,
                 **kwargs):
        super().__init__()
        if padding is None:
            padding = (kernel_size - 1) // 2
        if conv == 'conv':
            conv_layer = nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding)
        elif conv == 'deconv':
            # output = (input - 1)*stride + output_padding - 2 * padding + kernel_size
            output_padding = kwargs.get('output_padding', stride + 2 * padding - kernel_size)
            conv_layer = nn.ConvTranspose2d(in_channel, out_channel, kernel_size, stride, padding, output_padding)
        else:
            raise RuntimeError(f'Unknown conv layer : {conv}')

        if isinstance(act, bool) or act is None:
            if act:
                active = nn.ReLU()
            else:
                active = None
        elif isinstance(act, str):
            if act == 'relu':
                active = nn.ReLU()
            elif act == 'leaky':
                active = nn.LeakyReLU()
            else:
                raise RuntimeError(f'Unknown active: {act}')
        elif isinstance(act, nn.Module):
            active = act
        else:
            raise RuntimeError(f'Unknown active type: {type(act)}')

        if isinstance(bn, bool) or bn is None:
            if bn:
                norm = nn.BatchNorm2d(out_channel)
            else:
                norm = None
        elif isinstance(bn, str):
            if bn == 'bn' or bn is True:
                norm = nn.BatchNorm2d(out_channel)
            else:
                raise RuntimeError(f'Unknown batchnorm: {bn}')
        elif isinstance(bn, nn.Module):
            norm = bn
        else:
            raise RuntimeError(f'Unknown norm type: {type(bn)}')

        layers = [conv_layer]
        if act:
            layers.append(active)
        if bn:
            layers.append(norm)
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)


def conv_block(in_channels, out_channels, kernel_size, stride=1, dilation=1, bias=True, valid_padding=True, padding=0,
               act_type='relu', norm_type='bn', pad_type='zero', mode='CNA'):
    assert (mode in ['CNA', 'NAC']), '[ERROR] Wrong mode in [%s]!' % sys.modules[__name__]

    if valid_padding:
        padding = get_valid_padding(kernel_size, dilation)
    else:
        pass
    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None
    conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation,
                     bias=bias)

    if mode == 'CNA':
        act = activation(act_type) if act_type else None
        n = norm(out_channels, norm_type) if norm_type else None
        return sequential(p, conv, n, act)
    elif mode == 'NAC':
        act = activation(act_type, inplace=False) if act_type else None
        n = norm(in_channels, norm_type) if norm_type else None
        return sequential(n, act, p, conv)


class ResBlock(nn.Module):
    def __init__(self, in_channel, out_channle, mid_channel, kernel_size, stride=1, valid_padding=True, padding=0,
                 dilation=1, bias=True, pad_type='zero', norm_type='bn', act_type='relu', mode='CNA', res_scale=1):
        super().__init__()
        conv0 = conv_block(in_channel, mid_channel, kernel_size, stride, dilation, bias, valid_padding, padding,
                           act_type, norm_type, pad_type, mode)
        act_type = ''
        norm_type = ''
        conv1 = conv_block(mid_channel, out_channle, kernel_size, stride, dilation, bias, valid_padding, padding,
                           act_type, norm_type, pad_type, mode)
        self.res = sequential(conv0, conv1)
        self.res_scale = res_scale

    def forward(self, x):
        res = self.res(x) * self.res_scale
        return x + res


class ShortcutBlock(nn.Module):
    def __init__(self, submodule):
        super().__init__()
        self.sub = submodule

    def forward(self, x):
        output = x + self.sub(x)
        return output


class ConcatBlock(nn.Module):
    def __init__(self, submodule):
        super().__init__()
        self.sub = submodule

    def forward(self, x):
        output = t.cat((x, self.sub(x)), 1)
        return output


def upsample_conv_block(upscale_factor, in_channels, out_channels, kernel_size, stride, valid_padding=True, padding=0,
                        bias=True, pad_type='zero', act_type='relu', norm_type=None, mode='nearest'):
    upsample = nn.Upsample(scale_factor=upscale_factor, mode=mode)
    conv = conv_block(in_channels, out_channels, kernel_size, stride, bias=bias, valid_padding=valid_padding,
                      padding=padding, pad_type=pad_type, act_type=act_type, norm_type=norm_type)
    return sequential(upsample, conv)


def deconv_block(in_channels, out_channels, kernel_size, stride=1, dilation=1, bias=True, padding=0, act_type='relu',
                 norm_type='bn', pad_type='zero', mode='CNA'):
    assert (mode in ['CNA', 'NAC']), '[ERROR] Wrong mode in [%s]!' % sys.modules[__name__]

    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None
    deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, dilation=dilation, bias=bias)

    if mode == 'CNA':
        act = activation(act_type) if act_type else None
        n = norm(out_channels, norm_type) if norm_type else None
        return sequential(p, deconv, n, act)
    elif mode == 'NAC':
        act = activation(act_type, inplace=False) if act_type else None
        n = norm(in_channels, norm_type) if norm_type else None
        return sequential(n, act, p, deconv)


################
# helper funcs
################

def get_valid_padding(kernel_size, dilation):
    """
    Padding value to remain feature size.
    """
    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)
    padding = (kernel_size - 1) // 2
    return padding


def n_tuple(n):
    def parse(x):
        if isinstance(x, Iterable):
            return x
        return tuple(repeat(x, n))

    return parse


def locally_connected2d(x, weight, stride=1):
    b, c, h, w = x.size()
    height, width, out_channel, in_channel, kernel_size, _ = weight.shape
    x = nn.functional.unfold(x, kernel_size, padding=kernel_size // 2, stride=(stride, stride))
    x = x.reshape(b, c, kernel_size, kernel_size, h, w).permute(0, 1, 4, 5, 2, 3)
    out = (x.unsqueeze(1) * weight).sum([2, -2, -1])
    return out


class PixelUpSampler(nn.Sequential):
    def __init__(self, conv, scale, n_feats, bn=False, act=False, bias=True):

        m = []
        # Is scale = 2^n?
        if not (scale & (scale - 1)):
            for _ in range(int(math.log2(scale))):
                m.append(conv(n_feats, 4 * n_feats, 3, bias=bias))
                m.append(nn.PixelShuffle(2))
                if bn:
                    m.append(nn.BatchNorm2d(n_feats))
                if act == 'relu':
                    m.append(nn.ReLU())
                elif act == 'prelu':
                    m.append(nn.PReLU(n_feats))

        elif scale == 3:
            m.append(conv(n_feats, 9 * n_feats, 3, bias))
            m.append(nn.PixelShuffle(3))
            if bn:
                m.append(nn.BatchNorm2d(n_feats))
            if act == 'relu':
                m.append(nn.ReLU())
            elif act == 'prelu':
                m.append(nn.PReLU(n_feats))
        else:
            raise NotImplementedError

        super().__init__(*m)


class GaussianLayer(nn.Sequential):
    def __init__(self, channel=3, kernel_size=21, sigma=3):
        seq = [
            nn.ReflectionPad2d(kernel_size // 2),
            nn.Conv2d(channel, channel, kernel_size, stride=1, padding=0, bias=False, groups=channel)
        ]
        n = np.zeros((kernel_size, kernel_size))
        n[kernel_size // 2, kernel_size // 2] = 1
        k = gaussian_filter(n, sigma=sigma)
        seq[1].weight.data[0] = t.from_numpy(k)
        seq[1].weight.requires_grad = False

        super().__init__(*seq)


def default_conv(in_channels, out_channels, kernel_size, stride=1, bias=True):
    return nn.Conv2d(
        in_channels, out_channels, kernel_size,
        padding=(kernel_size // 2), stride=stride, bias=bias)
